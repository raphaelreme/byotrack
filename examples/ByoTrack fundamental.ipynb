{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb91ceb4",
   "metadata": {},
   "source": [
    "# ByoTrack fundamental features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ef391",
   "metadata": {},
   "source": [
    "## Loading a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from byotrack import Video, VideoTransformConfig\n",
    "\n",
    "TEST = True  # Set to False to analyze a whole video\n",
    "\n",
    "video_path = \"path/to/video.ext\"\n",
    "icy_path = \"path/to/icy_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply open a video\n",
    "video = Video(video_path)\n",
    "\n",
    "fps = 20\n",
    "# fps = video.reader.fps\n",
    "\n",
    "# Note: video could also be a 4 dimensionnal numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transform can be added to normalize and aggregate channels\n",
    "\n",
    "transform_config = VideoTransformConfig(aggregate=True, normalize=True, q_min=0.01, q_max=0.999)\n",
    "video.set_transform(transform_config)\n",
    "\n",
    "# Show the min max value used to clip and normalize\n",
    "print(video._normalizer.mini, video._normalizer.maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a79b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(video[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the whole video with opencv\n",
    "\n",
    "for i, frame in enumerate(video):\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f1cb4",
   "metadata": {},
   "source": [
    "## Detections on a video: Example of WaveletDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c718014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from byotrack.implementation.detector.wavelet import WaveletDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object with its hyper parameters\n",
    "\n",
    "detector = WaveletDetector(scale=1, k=3.0, min_area=3, batch_size=20, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c30c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detection process on the current video\n",
    "\n",
    "if TEST:  # Use slicing on video to run detection only on a part of it\n",
    "    detections_sequence = detector.run(video[:50])\n",
    "else:\n",
    "    detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first detections\n",
    "\n",
    "segmentation = detections_sequence[0].segmentation.clone()\n",
    "segmentation[segmentation!=0] += 50  # Improve visibility of firsts labels\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(segmentation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the detections with opencv\n",
    "# Use w and x to move backward/forward in the video\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    mask = (detections_sequence[i].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', mask)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(detections_sequence)} - Number of detections: {detections_sequence[i].length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(detections_sequence)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(detections_sequence)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbe66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters manually on the video\n",
    "# Use w/x to move backward/forward in the video\n",
    "# Use c/v to update k (the main hyperparameter)\n",
    "# You can restard with another scale/min_area\n",
    "\n",
    "K_SPEED = 0.01\n",
    "\n",
    "i = 0\n",
    "detector = WaveletDetector(scale=1, k=3.0, min_area=3.0, device=torch.device(\"cpu\"))\n",
    "\n",
    "while True:\n",
    "    frame = video[i]\n",
    "    detections = detector.detect(frame[None, ...])[0]\n",
    "    mask = (detections.segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', mask)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)} - k={detector.k} - Num detections: {detections.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(video)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(video)\n",
    "\n",
    "    if key == ord(\"c\"):\n",
    "        detector.k = detector.k * (1 - K_SPEED)\n",
    "\n",
    "    if key == ord(\"v\"):\n",
    "        detector.k = detector.k * (1 + K_SPEED)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1264d7",
   "metadata": {},
   "source": [
    "## Link detections: Example of IcyEMHTLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack import Track\n",
    "from byotrack.implementation.linker.icy_emht import IcyEMHTLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linker object with icy path\n",
    "# This Linker requires to install Icy software first\n",
    "\n",
    "linker = IcyEMHTLinker(icy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de479b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set the expected motion of particles\n",
    "\n",
    "linker.motion = linker.Motion.BROWNIAN  # Already by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the linker given a video (Unused) and detections\n",
    "\n",
    "if TEST:  # Use slicing to run linker only on a part of the data\n",
    "    tracks = linker.run(video, detections_sequence[:50])\n",
    "else:\n",
    "    tracks = linker.run(video, detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905634f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks with opencv\n",
    "# Use w/x to move backward/forward in the video (or space to run the video)\n",
    "# Use c to display also the detections on the frame\n",
    "\n",
    "fps = 20\n",
    "running = False\n",
    "display_detections = False\n",
    "\n",
    "frame_id = 0\n",
    "\n",
    "while True:\n",
    "    frame_id += running\n",
    "    frame = (video[frame_id] * 255).astype(np.uint8)\n",
    "    if display_detections and frame_id < len(detections_sequence):\n",
    "        mask = (detections_sequence[frame_id].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "        frame = np.concatenate((mask[..., None], frame, np.zeros_like(frame)), axis=2)\n",
    "    else:\n",
    "        frame = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=2)\n",
    "\n",
    "    # Add tracklets\n",
    "    for track in tracks:\n",
    "        point = track[frame_id]\n",
    "        if torch.isnan(point).any():\n",
    "            continue\n",
    "\n",
    "        x, y = point.round().to(torch.int).tolist()\n",
    "\n",
    "        color = (0, 0, 255)  # Red\n",
    "\n",
    "        cv2.circle(frame, (x, y), 5, color)\n",
    "        cv2.putText(frame, str(track.identifier % 10), (x + 4, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {frame_id} / {len(video)}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\" \"):\n",
    "        running = not running\n",
    "\n",
    "    if not running and key == ord(\"w\"):  # Prev\n",
    "        frame_id = (frame_id - 1) % len(video)\n",
    "\n",
    "    if not running and key == ord(\"x\"):  # Next\n",
    "        frame_id = (frame_id + 1) % len(video)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        display_detections = 1 - display_detections\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87805b9",
   "metadata": {},
   "source": [
    "## Tracks refinement: Example of Cleaner, followed by EMC2 Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55757b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.implementation.refiner.cleaner import Cleaner\n",
    "from byotrack.implementation.refiner.stitching import EMC2Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tracks with consecutive dist > 3.5\n",
    "# Drop tracks with length < 5\n",
    "\n",
    "cleaner = Cleaner(min_length=5, max_dist=3.5)\n",
    "tracks = cleaner.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch tracks together in order to produce coherent track on all the video\n",
    "\n",
    "stitcher = EMC2Stitcher(eta=5.0)  # Don't link tracks if they are too far (EMC dist > 5 (pixels))\n",
    "tracks = stitcher.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c37ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59009a",
   "metadata": {},
   "source": [
    "## End-to-end tracking: Example of MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack import MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the steps: Detector, Linker[, Refiner]\n",
    "# Then the tracker\n",
    "\n",
    "detector = WaveletDetector(scale=1, k=3, min_area=3.0, batch_size=20, device=torch.device(\"cpu\"))\n",
    "linker = IcyEMHTLinker(icy_path)\n",
    "\n",
    "# Optional refiner\n",
    "refiners = []\n",
    "if True:\n",
    "    refiners = [Cleaner(5, 3.5), EMC2Stitcher(eta=5.0)]\n",
    "\n",
    "tracker = MultiStepTracker(detector, linker, refiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:  # Use slicing on video to run tracker only on a part of it\n",
    "    tracks = tracker.run(video[:50])\n",
    "else:\n",
    "    tracks = tracker.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6edc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks with opencv\n",
    "# Use w/x to move backward/forward in the video (or space to run the video)\n",
    "# Use c to display also the detections on the frame\n",
    "\n",
    "fps = 20\n",
    "running = False\n",
    "display_detections = False\n",
    "\n",
    "frame_id = 0\n",
    "\n",
    "while True:\n",
    "    frame_id += running\n",
    "    frame = (video[frame_id] * 255).astype(np.uint8)\n",
    "    if display_detections and frame_id < len(detections_sequence):\n",
    "        mask = (detections_sequence[frame_id].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "        frame = np.concatenate((mask[..., None], frame, np.zeros_like(frame)), axis=2)\n",
    "    else:\n",
    "        frame = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=2)\n",
    "\n",
    "    # Add tracklets\n",
    "    for track in tracks:\n",
    "        point = track[frame_id]\n",
    "        if torch.isnan(point).any():\n",
    "            continue\n",
    "\n",
    "        x, y = point.round().to(torch.int).tolist()\n",
    "\n",
    "        color = (0, 0, 255)  # Red\n",
    "\n",
    "        cv2.circle(frame, (x, y), 5, color)\n",
    "        cv2.putText(frame, str(track.identifier % 10), (x + 4, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {frame_id} / {len(video)}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(1000 // fps) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\" \"):\n",
    "        running = not running\n",
    "\n",
    "    if not running and key == ord(\"w\"):  # Prev\n",
    "        frame_id = (frame_id - 1) % len(video)\n",
    "\n",
    "    if not running and key == ord(\"x\"):  # Next\n",
    "        frame_id = (frame_id + 1) % len(video)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        display_detections = 1 - display_detections\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
