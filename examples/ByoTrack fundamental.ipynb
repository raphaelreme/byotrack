{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb91ceb4",
   "metadata": {},
   "source": [
    "# ByoTrack fundamental features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import byotrack\n",
    "import byotrack.visualize\n",
    "\n",
    "\n",
    "TEST = True  # Set to False to analyze a whole video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ef391",
   "metadata": {},
   "source": [
    "## Loading a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"path/to/video.ext\"\n",
    "\n",
    "# Simply open a video\n",
    "video = byotrack.Video(video_path)\n",
    "\n",
    "fps = 20\n",
    "# fps = video.reader.fps\n",
    "\n",
    "# Note: video could also be a 4 dimensionnal numpy array loaded manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transform can be added to normalize and aggregate channels\n",
    "\n",
    "transform_config = byotrack.VideoTransformConfig(aggregate=True, normalize=True, q_min=0.01, q_max=0.995, smooth_clip=1.0)\n",
    "video.set_transform(transform_config)\n",
    "\n",
    "# Show the min max value used to clip and normalize\n",
    "print(video._normalizer.mini, video._normalizer.maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a79b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(video[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689677a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f1cb4",
   "metadata": {},
   "source": [
    "## Detections on a video: Example of WaveletDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object with its hyper parameters\n",
    "from byotrack.implementation.detector.wavelet import WaveletDetector\n",
    "\n",
    "detector = WaveletDetector(scale=1, k=3.0, min_area=3.0, batch_size=20, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c30c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detection process on the current video\n",
    "\n",
    "if TEST:  # Use slicing on video to run detection only on a part of it\n",
    "    detections_sequence = detector.run(video[:50])\n",
    "else:\n",
    "    detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first detections\n",
    "\n",
    "segmentation = detections_sequence[0].segmentation.clone()\n",
    "segmentation[segmentation!=0] += 50  # Improve visibility of firsts labels\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(segmentation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the detections with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v to switch on/off the display of the video\n",
    "# Use d to switch detection display mode (None, mask, segmentation)\n",
    "\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video, detections_sequence).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbe66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters manually on the video\n",
    "# Use w/x to move backward/forward in the video\n",
    "# Use c/v to update k (the main hyperparameter)\n",
    "# You can restard with another scale/min_area\n",
    "\n",
    "K_SPEED = 0.01\n",
    "\n",
    "i = 0\n",
    "detector = WaveletDetector(scale=1, k=3.0, min_area=3.0, device=torch.device(\"cpu\"))\n",
    "\n",
    "while True:\n",
    "    frame = video[i]\n",
    "    detections = detector.detect(frame[None, ...])[0]\n",
    "    mask = (detections.segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', mask)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)} - k={detector.k} - Num detections: {detections.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(video)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(video)\n",
    "\n",
    "    if key == ord(\"c\"):\n",
    "        detector.k = detector.k * (1 - K_SPEED)\n",
    "\n",
    "    if key == ord(\"v\"):\n",
    "        detector.k = detector.k * (1 + K_SPEED)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1264d7",
   "metadata": {},
   "source": [
    "## Link detections: Example of IcyEMHTLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.implementation.linker.icy_emht import IcyEMHTLinker, Motion, EMHTParameters\n",
    "\n",
    "# Create the linker object with icy path\n",
    "# This Linker requires to install Icy software first\n",
    "\n",
    "icy_path = \"path/to/icy/icy.jar\"\n",
    "motion = Motion.BROWNIAN  # Already by default. Can also be DIRECTED or MULTI (both)\n",
    "\n",
    "if True:  # You can choose to set manually the parameters. See EMHTParameters?\n",
    "    parameters = EMHTParameters(gate_factor=5.0, motion=motion)\n",
    "    linker = IcyEMHTLinker(icy_path, parameters)\n",
    "else:\n",
    "    linker = IcyEMHTLinker(icy_path)\n",
    "    linker.motion = motion  # Set motion afterwards is no parameters are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the linker given a video (Unused) and detections\n",
    "\n",
    "if TEST:  # Use slicing to run linker only on a part of the data\n",
    "    tracks = linker.run(video, detections_sequence[:50])\n",
    "else:\n",
    "    tracks = linker.run(video, detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1add03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tracks with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v (resp. t) to switch on/off the display of video (resp. tracks)\n",
    "# Use d to switch detection display mode (None, mask, segmentation)\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video, detections_sequence, tracks).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87805b9",
   "metadata": {},
   "source": [
    "## Tracks refinement: Example of Cleaner, followed by EMC2 Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55757b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.implementation.refiner.cleaner import Cleaner\n",
    "from byotrack.implementation.refiner.stitching import EMC2Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tracks with consecutive dist > 3.5\n",
    "# Drop tracks with length < 5\n",
    "\n",
    "cleaner = Cleaner(min_length=5, max_dist=3.5)\n",
    "tracks = cleaner.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch tracks together in order to produce coherent track on all the video\n",
    "\n",
    "stitcher = EMC2Stitcher(eta=5.0)  # Don't link tracks if they are too far (EMC dist > 5 (pixels))\n",
    "tracks = stitcher.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c37ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59009a",
   "metadata": {},
   "source": [
    "## End-to-end tracking: Example of MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack import MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the steps: Detector, Linker[, Refiner]\n",
    "# Then the tracker\n",
    "\n",
    "detector = WaveletDetector(scale=1, k=3, min_area=3.0, batch_size=20, device=torch.device(\"cpu\"))\n",
    "linker = IcyEMHTLinker(icy_path)\n",
    "\n",
    "# Optional refiner\n",
    "refiners = []\n",
    "if True:\n",
    "    refiners = [Cleaner(5, 3.5), EMC2Stitcher(eta=5.0)]\n",
    "\n",
    "tracker = MultiStepTracker(detector, linker, refiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:  # Use slicing on video to run tracker only on a part of it\n",
    "    tracks = tracker.run(video[:50])\n",
    "else:\n",
    "    tracks = tracker.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6edc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tracks with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v (resp. t) to switch on/off the display of video (resp. tracks)\n",
    "# Use d to switch detection display mode (None, mask, segmentation)\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video, detections_sequence, tracks).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7fb4c5",
   "metadata": {},
   "source": [
    "## Postprocessing: Fill NaN with interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.implementation.refiner.interpolater import ForwardBackwardInterpolater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e11f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After EMC2 stitching, NaN values can be inside merged tracks.\n",
    "# It can be filled with interpolation between known positions\n",
    "\n",
    "# Note that you can add this refiner to your MultiStepTracker pipeline\n",
    "\n",
    "method = \"constant\"  # tps / constant\n",
    "full = False  # Extrapolate position of the tracks on the all frame range and not just for the track lifespan\n",
    "\n",
    "tracks = ForwardBackwardInterpolater(method, full=False).run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa43b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c8205",
   "metadata": {},
   "source": [
    "## Load or save tracks to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tracks in ByoTrack format (compressed in a torch tensor)\n",
    "\n",
    "byotrack.Track.save(tracks, \"tracks.pth\")\n",
    "\n",
    "# Can be reload with\n",
    "tracks = byotrack.Track.load(\"tracks.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also provide IO with Icy software\n",
    "\n",
    "from byotrack import icy\n",
    "\n",
    "\n",
    "icy.save_tracks(tracks, \"tracks.xml\")  # Note that holes should should be filled first with the ForwardBackwardInterpolater\n",
    "\n",
    "# You can (re)load tracks from icy with\n",
    "tracks = icy.load_tracks(\"tracks.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
