{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb91ceb4",
   "metadata": {},
   "source": [
    "# ByoTrack fundamental features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import byotrack\n",
    "import byotrack.example_data\n",
    "import byotrack.visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ef391",
   "metadata": {},
   "source": [
    "## Loading a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an example video\n",
    "video = byotrack.example_data.hydra_neurons()[130:]  # Let's start at frame 130 where the animal is contracting\n",
    "\n",
    "# Or provide a path to one of your video\n",
    "# video = byotrack.Video(\"path/to/video.ext\")\n",
    "\n",
    "# Or load manually a video as a numpy array\n",
    "# video = np.random.randn(50, 500, 500, 3)  # T, H, W, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True  # Set to False to analyze a whole video\n",
    "\n",
    "if TEST:\n",
    "    video = video[:50]  # Temporal slicing to analyze only the first 50 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For video only (With numpy arrays, your are responsible for channels aggregation and normalization)\n",
    "# A transform can be added to normalize and aggregate channels\n",
    "\n",
    "transform_config = byotrack.VideoTransformConfig(\n",
    "    aggregate=True, normalize=True, q_min=0.01, q_max=0.999, smooth_clip=1.0\n",
    ")\n",
    "video.set_transform(transform_config)\n",
    "\n",
    "# Show the min max value used to clip and normalize\n",
    "print(video._normalizer.mini, video._normalizer.maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a79b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(video[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10299129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v to switch on/off the display of the video\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f1cb4",
   "metadata": {},
   "source": [
    "## Detections on a video: Example of WaveletDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object with its hyper parameters\n",
    "from byotrack.implementation.detector.wavelet import WaveletDetector\n",
    "\n",
    "detector = WaveletDetector(scale=1, k=3.0, min_area=5.0, batch_size=20, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c30c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detection process on the current video\n",
    "\n",
    "detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first detections\n",
    "\n",
    "segmentation = detections_sequence[0].segmentation.clone()\n",
    "segmentation[segmentation!=0] += 50  # Improve visibility of firsts labels\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(segmentation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the detections with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v to switch on/off the display of the video\n",
    "# Use d to switch detection display mode (None, mask, segmentation)\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video, detections_sequence).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbe66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters manually on the video\n",
    "# Use w/x to move backward/forward in the video\n",
    "# Use c/v to update k (the main hyperparameter)\n",
    "# You can restard with another scale/min_area\n",
    "\n",
    "K_SPEED = 0.01\n",
    "\n",
    "i = 0\n",
    "detector = WaveletDetector(scale=1, k=3.0, min_area=5.0, device=torch.device(\"cpu\"))\n",
    "\n",
    "while True:\n",
    "    frame = video[i]\n",
    "    detections = detector.detect(frame[None, ...])[0]\n",
    "    mask = (detections.segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', mask)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)} - k={detector.k} - Num detections: {detections.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(video)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(video)\n",
    "\n",
    "    if key == ord(\"c\"):\n",
    "        detector.k = detector.k * (1 - K_SPEED)\n",
    "\n",
    "    if key == ord(\"v\"):\n",
    "        detector.k = detector.k * (1 + K_SPEED)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1264d7",
   "metadata": {},
   "source": [
    "## Link detections: Example of KOFTLinker (Kalman and Optical Flow Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOFT requires Optical Flow. We give here the example of Farneback from Open-CV.\n",
    "\n",
    "from byotrack.implementation.linker.frame_by_frame.koft import KOFTLinker, KOFTLinkerParameters\n",
    "from byotrack.implementation.optical_flow.opencv import OpenCVOpticalFlow\n",
    "\n",
    "# Prepare the optical flow algorithm\n",
    "optflow = OpenCVOpticalFlow(cv2.FarnebackOpticalFlow_create(winSize=20), downscale=4)\n",
    "\n",
    "# Create the linker\n",
    "# Look at the documentation (KOFTLinkerParameters?) to see what parameters are available and their full descriptions\n",
    "\n",
    "specs = KOFTLinkerParameters(\n",
    "    association_threshold=1e-3,  # Most important parameter: don't link if the association likelihood is smaller than 1e-3.\n",
    "    detection_std=3.0,  # Detections location are precise up to 3.0 pixels (Usually ~ size of spots)\n",
    "    process_std=1.5,  # Kalman filter predictions are precise up to 1.5 pixels (Usually ~ size of unmodeled displacement)\n",
    "    flow_std=1.0,  # Optical flow predictions are precise up to 1.0 pixels/frame\n",
    "    kalman_order=1,  # Order of the kalman filter (1: Directed, 2: Accelerated, ...)\n",
    "    n_gap=5,  # Allow to link after 5 consecutive missed detections\n",
    "    cost=\"likelihood\",  # See koft.Cost? to see which other cost are available, by default it uses Euclidean distance (And association threshod should be express in pixels)\n",
    ")\n",
    "\n",
    "linker = KOFTLinker(specs, optflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da01d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before linking, let's check visually that the optical flow algorithm works\n",
    "# We sample a grid of points that are moved by the flow computed.\n",
    "# The computed flows are good if the points roughly follows the video motion\n",
    "\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use g to reset the grid of points\n",
    "\n",
    "byotrack.visualize.InteractiveFlowVisualizer(video, optflow).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the linker given a video and detections\n",
    "\n",
    "tracks = linker.run(video, detections_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "# Each track is in white when it alive. (Track on x-axis, time on y-axis)\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ccefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project tracks onto a single image and color by time\n",
    "\n",
    "# Create a list of colors for each time frame\n",
    "# From cyan (start of the video) to red (end of the video)\n",
    "\n",
    "hsv = mpl.colormaps[\"hsv\"]\n",
    "colors = [tuple(int(c * 255) for c in hsv(0.5 + 0.5 * k / (len(detections_sequence) - 1))[:3]) for k in range(len(detections_sequence))]\n",
    "\n",
    "visu = byotrack.visualize.temporal_projection(\n",
    "    byotrack.Track.tensorize(tracks),\n",
    "    colors=colors,\n",
    "    background=video[0],\n",
    "    color_by_time=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    "plt.imshow(visu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1add03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tracks with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v (resp. t) to switch on/off the display of video (resp. tracks)\n",
    "# Use d to switch detection display mode (None, mask, segmentation)\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video, detections_sequence, tracks).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87805b9",
   "metadata": {},
   "source": [
    "## Tracks refinement: Example of Cleaner, followed by EMC2 Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55757b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.implementation.refiner.cleaner import Cleaner\n",
    "from byotrack.implementation.refiner.interpolater import ForwardBackwardInterpolater\n",
    "from byotrack.implementation.refiner.stitching import EMC2Stitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tracks with consecutive dist > 5. Splitting may be counterproductive, check the tracks before applying such cleaner.\n",
    "# Drop tracks with length < 5\n",
    "\n",
    "cleaner = Cleaner(min_length=5, max_dist=5.)\n",
    "tracks = cleaner.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch tracks together in order to produce coherent track on all the video\n",
    "\n",
    "stitcher = EMC2Stitcher(eta=5.0)  # Don't link tracks if they are too far (EMC dist > 5 (pixels))\n",
    "tracks = stitcher.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c37ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573906b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After EMC2 stitching, NaN values can be inside merged tracks.\n",
    "# It can be filled with interpolation between known positions\n",
    "\n",
    "method = \"tps\"  # tps / constant / flow (You need to provided a valid byotrack.OpticalFlow then)\n",
    "full = False  # Extrapolate position of the tracks on the all frame range and not just for the track lifespan\n",
    "\n",
    "interpolater = ForwardBackwardInterpolater(method, full)\n",
    "tracks = interpolater.run(video, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project tracks onto a single image and color by time\n",
    "\n",
    "# Create a list of colors for each time frame\n",
    "# From cyan (start of the video) to red (end of the video)\n",
    "\n",
    "hsv = mpl.colormaps[\"hsv\"]\n",
    "colors = [tuple(int(c * 255) for c in hsv(0.5 + 0.5 * k / (len(detections_sequence) - 1))[:3]) for k in range(len(detections_sequence))]\n",
    "\n",
    "visu = byotrack.visualize.temporal_projection(\n",
    "    byotrack.Track.tensorize(tracks),\n",
    "    colors=colors,\n",
    "    background=video[0],\n",
    "    color_by_time=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    "plt.imshow(visu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59009a",
   "metadata": {},
   "source": [
    "## End-to-end tracking - Online or Offline tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack import BatchMultiStepTracker, MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full tracking pipeline from detector, linker and refiners\n",
    "# If you have a BatchDetector and a OnlineLinker (True for WaveletDetector and KOFTLinker)\n",
    "# You may use BatchMultiStepTracker that will process online the video (never keeping all the segmentations in RAM)\n",
    "# Otherwise, use MultiStepTracker (Run Detections, then linking)\n",
    "\n",
    "tracker = BatchMultiStepTracker(detector, linker, (cleaner, stitcher, interpolater))\n",
    "# tracker = MultiStepTracker(detector, linker, (cleaner, stitcher, interpolater))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracker.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6edc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize track lifetime\n",
    "\n",
    "byotrack.visualize.display_lifetime(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project tracks onto a single image and color by track\n",
    "\n",
    "# Create a list of colors for each track (if more tracks than colors, it will cycle through it)\n",
    "\n",
    "hsv = mpl.colormaps[\"hsv\"]\n",
    "colors = [tuple(int(c * 255) for c in hsv(k / 199)[:3]) for k in range(200)]\n",
    "\n",
    "visu = byotrack.visualize.temporal_projection(\n",
    "    byotrack.Track.tensorize(tracks),\n",
    "    colors=colors,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    "plt.imshow(visu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tracks with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use v (resp. t) to switch on/off the display of video (resp. tracks)\n",
    "# Use d to switch detection display mode (None, mask, segmentation)\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video, detections_sequence, tracks).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c8205",
   "metadata": {},
   "source": [
    "## Load or save tracks to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tracks in ByoTrack format (compressed in a torch tensor)\n",
    "\n",
    "byotrack.Track.save(tracks, \"tracks.pt\")\n",
    "\n",
    "# Can be reload with\n",
    "tracks = byotrack.Track.load(\"tracks.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also provide IO with Icy software\n",
    "\n",
    "from byotrack import icy\n",
    "\n",
    "\n",
    "icy.save_tracks(tracks, \"tracks.xml\")  # Note that holes should should be filled first with the ForwardBackwardInterpolater\n",
    "\n",
    "# You can (re)load tracks from icy with\n",
    "tracks = icy.load_tracks(\"tracks.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
