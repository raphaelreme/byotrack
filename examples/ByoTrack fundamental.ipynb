{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb91ceb4",
   "metadata": {},
   "source": [
    "# ByoTrack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ef391",
   "metadata": {},
   "source": [
    "## Loading a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from byotrack.video.reader import VideoReader\n",
    "from byotrack.video.transforms import VideoTransformConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply open a video\n",
    "video = VideoReader.open(\"path/to/video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transform can be added to normalize and aggregate channels\n",
    "\n",
    "transform_config = VideoTransformConfig(normalize=True, q_min=0.01, q_max=0.999)\n",
    "video.set_transform(transform_config)\n",
    "\n",
    "# Show the min max value used to clip and normalize\n",
    "print(video.transform.normalizer.mini, video.transform.normalizer.maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a79b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(video.retrieve())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the whole video with opencv\n",
    "\n",
    "has_next = True\n",
    "while has_next:\n",
    "    frame, has_next = video.read()\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {video.frame_id} / {video.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(video.fps if video.fps != -1 else 20) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f1cb4",
   "metadata": {},
   "source": [
    "## Detections on a video: Example of SpotDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c718014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from byotrack.detector.spot_detector import SpotDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object with its hyper parameters\n",
    "\n",
    "detector = SpotDetector(scale=1, k = 3.0, min_area=5, batch_size=50, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c30c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detection process on the current video\n",
    "\n",
    "detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the detections with opencv\n",
    "# Use w and x to move backward/forward in the video\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    mask = (detections_sequence[i].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', mask)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(detections_sequence)} - Number of detections: {detections_sequence[i].length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(video.fps if video.fps != -1 else 20) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "        \n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(detections_sequence)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(detections_sequence)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbe66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters manually on the video\n",
    "# Use w/x to move backward/forward in the video\n",
    "# Use c/v to update k (the main hyperparameter)\n",
    "# You can restard with another scale/min_area\n",
    "\n",
    "K_SPEED = 0.01\n",
    "\n",
    "i = 0\n",
    "video.seek(i)\n",
    "frame = video.retrieve()\n",
    "\n",
    "detector = SpotDetector(scale=1, k = 3.0, min_area=5.0, device=torch.device(\"cpu\"))\n",
    "\n",
    "while True:\n",
    "    detections = detector.detect(frame[None, ...])[0]\n",
    "    mask = (detections.segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', mask)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {video.length} - k={detector.k} - Num detections: {detections.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "        \n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % video.length\n",
    "        video.seek(i)\n",
    "        frame = video.retrieve()\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % video.length\n",
    "        video.seek(i)\n",
    "        frame = video.retrieve()\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        detector.k = detector.k * (1 - K_SPEED)\n",
    "\n",
    "    if key == ord(\"v\"):\n",
    "        detector.k = detector.k * (1 + K_SPEED)\n",
    "    \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1264d7",
   "metadata": {},
   "source": [
    "## Link detections: Example of IcyEMHTLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack import Track\n",
    "from byotrack.linker.icy_emht.icy_emht import IcyEMHTLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linker object with icy path\n",
    "linker = IcyEMHTLinker(\"path/to/icy/dir/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de479b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set the expected motion of particles\n",
    "\n",
    "linker.motion = linker.Motion.BROWNIAN  # Already by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the linker given a video (Unused) and detections\n",
    "\n",
    "tracks = linker.run(None, detections_sequence[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905634f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks with opencv\n",
    "# Use w/x to move backward/forward in the video (or space to run the video)\n",
    "\n",
    "fps = 20\n",
    "running = False\n",
    "\n",
    "video.seek(0)\n",
    "true_frame = video.retrieve()\n",
    "\n",
    "while True:\n",
    "    if running:\n",
    "        true_frame, has_next = video.read()\n",
    "\n",
    "    frame = true_frame.copy()\n",
    "\n",
    "    # Add tracklets\n",
    "    for track in tracks:\n",
    "        point = track[video.frame_id]\n",
    "        if torch.isnan(point).any():\n",
    "            continue\n",
    "            \n",
    "        x, y = point.round().to(torch.int).tolist()\n",
    "\n",
    "        color = (0, 0, 255)  # Red\n",
    "\n",
    "        cv2.circle(frame, (x, y), 5, color)\n",
    "        cv2.putText(frame, str(track.identifier % 10), (x + 4, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {video.frame_id} / {video.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(int(1/fps * 1000)) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\" \"):\n",
    "        running = not running\n",
    "\n",
    "    if not running and key == ord(\"w\"):  # Prev\n",
    "        try:\n",
    "            video.seek(video.frame_id - 4)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        true_frame = video.retrieve()\n",
    "\n",
    "    if not running and key == ord(\"x\"):  # Next\n",
    "        true_frame, has_next = video.read()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59009a",
   "metadata": {},
   "source": [
    "## End-to-end tracking: Example of MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byotrack.tracker import MultiStepTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the steps: Detector, Linker[, Refiner]\n",
    "# Then the tracker\n",
    "\n",
    "detector = SpotDetector(scale=1, k = 3.0, min_area=5, batch_size=50, device=torch.device(\"cpu\"))\n",
    "linker = IcyEMHTLinker(\"path/to/icy/dir\")\n",
    "\n",
    "tracker = MultiStepTracker(detector, linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracker.run(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6edc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks existence in time\n",
    "\n",
    "# Transform into tensor\n",
    "tracks_tensor = Track.tensorize(tracks)\n",
    "print(tracks_tensor.shape)  # N_frame x N_track x D\n",
    "\n",
    "mask = ~ torch.isnan(tracks_tensor).any(dim=2)\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.xlabel(\"Track id\")\n",
    "plt.ylabel(\"Frame\")\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracks with opencv\n",
    "# Use w/x to move backward/forward in the video (or space to run the video)\n",
    "\n",
    "fps = 20\n",
    "running = False\n",
    "\n",
    "video.seek(0)\n",
    "true_frame = video.retrieve()\n",
    "\n",
    "while True:\n",
    "    if running:\n",
    "        true_frame, has_next = video.read()\n",
    "\n",
    "    frame = true_frame.copy()\n",
    "\n",
    "    # Add tracklets\n",
    "    for track in tracks:\n",
    "        point = track[video.frame_id]\n",
    "        if torch.isnan(point).any():\n",
    "            continue\n",
    "            \n",
    "        x, y = point.round().to(torch.int).tolist()\n",
    "\n",
    "        color = (0, 0, 255)  # Red\n",
    "\n",
    "        cv2.circle(frame, (x, y), 5, color)\n",
    "        cv2.putText(frame, str(track.identifier % 10), (x + 4, y - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.3, color)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {video.frame_id} / {video.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey(int(1/fps * 1000)) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "\n",
    "    if key == ord(\" \"):\n",
    "        running = not running\n",
    "\n",
    "    if not running and key == ord(\"w\"):  # Prev\n",
    "        try:\n",
    "            video.seek(video.frame_id - 4)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        true_frame = video.retrieve()\n",
    "\n",
    "    if not running and key == ord(\"x\"):  # Next\n",
    "        true_frame, has_next = video.read()\n",
    "\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
