{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca01ba2",
   "metadata": {},
   "source": [
    "# Detectors\n",
    "\n",
    "Review of all the detectors implemented in ByoTrack\n",
    "___________________________________________________\n",
    "\n",
    "1. **Spot Detector** (Wavelet filtering)\n",
    "2. **StarDist** (Unet + StarConvex prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9740b7b",
   "metadata": {},
   "source": [
    "## Load a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from byotrack import Video, VideoTransformConfig\n",
    "\n",
    "TEST = True  # Set to False to analyze a whole video\n",
    "\n",
    "video_path = \"path/to/video.ext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply open a video\n",
    "video = Video(video_path)\n",
    "\n",
    "fps = 20\n",
    "# fps = video.reader.fps\n",
    "\n",
    "# Note: video could also be a 4 dimensionnal numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transform can be added to normalize and aggregate channels\n",
    "\n",
    "transform_config = VideoTransformConfig(aggregate=True, normalize=True, q_min=0.02, q_max=0.999)\n",
    "video.set_transform(transform_config)\n",
    "\n",
    "# Show the min max value used to clip and normalize\n",
    "print(video._normalizer.mini, video._normalizer.maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(video[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad343a87",
   "metadata": {},
   "source": [
    "## SpotDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eee7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from byotrack.implementation.detector.spot_detector import SpotDetector\n",
    "\n",
    "SpotDetector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object with its hyper parameters. \n",
    "# The most important ones being the scale (size of the spots) and k the threshold noise\n",
    "\n",
    "detector = SpotDetector(scale=1, k=3.0, min_area=3, batch_size=20, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f199c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters manually on the video\n",
    "# Use w/x to move backward/forward in the video\n",
    "# Use c/v to update k (noise threshold)\n",
    "# Use b/n to update the scale (expected size of the spots)\n",
    "# You can also use the min_area to filter more or less spots given their area\n",
    "\n",
    "K_SPEED = 0.01\n",
    "scale = 1\n",
    "\n",
    "i = 0\n",
    "detector = SpotDetector(scale=scale, k=3.0, min_area=3, device=torch.device(\"cpu\"))\n",
    "\n",
    "while True:\n",
    "    frame = video[i]\n",
    "\n",
    "    # Run detection on a single frame using detect\n",
    "    detections = detector.detect(frame[None, ...])[0]\n",
    "    mask = (detections.segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "    \n",
    "    image = np.concatenate((frame, np.zeros_like(frame), mask[..., None]), axis=-1)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', image)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)} - scale={scale}, k={detector.k} - Num detections: {detections.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "        \n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(video)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(video)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        detector.k = detector.k * (1 - K_SPEED)\n",
    "\n",
    "    if key == ord(\"v\"):\n",
    "        detector.k = detector.k * (1 + K_SPEED)\n",
    "        \n",
    "    if key == ord(\"b\"):\n",
    "        scale = max(0, scale - 1)\n",
    "        detector = SpotDetector(scale, detector.k, detector.min_area, detector.device)\n",
    "        \n",
    "    \n",
    "    if key == ord(\"n\"):\n",
    "        scale = min(4, scale + 1)\n",
    "        detector = SpotDetector(scale, detector.k, detector.min_area, detector.device)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detection process on the current video\n",
    "\n",
    "if TEST:  # Use slicing on video to run detection only on a part of it\n",
    "    detections_sequence = detector.run(video[:50])\n",
    "else:\n",
    "    detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da573cbb",
   "metadata": {},
   "source": [
    "## StarDist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af566665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from byotrack.implementation.detector.stardist import StarDistDetector\n",
    "\n",
    "StarDistDetector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the detector object from a trained model. Note that the training should be done with\n",
    "# the official implementation (https://github.com/stardist/stardist).\n",
    "# Hyperparameters are set during the training phase (nms_treshold and prob_threshold)\n",
    "# They can be changed manually (See next cells)\n",
    "\n",
    "model_path = \"path/to/trained/model/\"\n",
    "detector = StarDistDetector(model_path, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters manually on the video\n",
    "# Use w/x to move backward/forward in the video\n",
    "# Use c/v to update prob_threshold (The most probable spots are kept)\n",
    "# Use b/n to update the nms_threshold (Delete overlapping spots)\n",
    "# You can also the min_area to filter more or less spots given their area\n",
    "\n",
    "prob_speed = 0.1\n",
    "nms_speed = 0.2\n",
    "\n",
    "i = 0\n",
    "detector = StarDistDetector(model_path, batch_size=5)\n",
    "\n",
    "while True:\n",
    "    frame = video[i]\n",
    "\n",
    "    # Run detection on a single frame using detect\n",
    "    detections = detector.detect(frame[None, ...])[0]\n",
    "    mask = (detections.segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "\n",
    "    image = np.concatenate((frame, np.zeros_like(frame), mask[..., None]), axis=-1)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', image)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(video)} - prob={detector.prob_threshold}, nms={detector.nms_threshold} - Num detections: {detections.length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "        \n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(video)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(video)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        detector.prob_threshold = detector.prob_threshold * (1 - prob_speed)\n",
    "\n",
    "    if key == ord(\"v\"):\n",
    "        detector.prob_threshold = detector.prob_threshold * (1 + prob_speed)\n",
    "\n",
    "    if key == ord(\"b\"):\n",
    "        detector.nms_threshold = detector.nms_threshold * (1 - nms_speed)\n",
    "\n",
    "    if key == ord(\"n\"):\n",
    "        detector.nms_threshold = detector.nms_threshold * (1 + nms_speed)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the detection process on the current video\n",
    "\n",
    "if TEST:  # Use slicing on video to run detection only on a part of it\n",
    "    detections_sequence = detector.run(video[:50])\n",
    "else:\n",
    "    detections_sequence = detector.run(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1770513",
   "metadata": {},
   "source": [
    "## Visualize the detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first detections\n",
    "\n",
    "segmentation = detections_sequence[0].segmentation.clone()\n",
    "segmentation[segmentation!=0] += 50  # Improve visibility of firsts labels\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(segmentation)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da74f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the detections with opencv\n",
    "# Use w and x to move backward/forward in the video\n",
    "# Use c to switch display mode (None / mask / segmentation)\n",
    "\n",
    "display = 1\n",
    "i = 0\n",
    "while True:\n",
    "    mask = (detections_sequence[i].segmentation.numpy() != 0).astype(np.uint8) * 255\n",
    "    segmentation = detections_sequence[i].segmentation.clone().numpy()\n",
    "    segmentation = (segmentation % 206) + 50\n",
    "    segmentation[detections_sequence[i].segmentation == 0] = 0\n",
    "    segementation = segmentation.astype(np.uint8)\n",
    "    frame = (video[i] * 255).round().astype(np.uint8)\n",
    "    if display == 0:\n",
    "        image = np.concatenate((np.zeros_like(frame), frame, np.zeros_like(frame)), axis=-1)\n",
    "    elif display == 1:\n",
    "        image = np.concatenate((np.zeros_like(frame), frame, mask[..., None]), axis=-1)\n",
    "    else:\n",
    "        image = np.concatenate((np.zeros_like(frame), frame, segementation[..., None]), axis=-1)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', image)\n",
    "    cv2.setWindowTitle('Frame', f'Frame {i} / {len(detections_sequence)} - Number of detections: {detections_sequence[i].length}')\n",
    "\n",
    "    # Press Q on keyboard to  exit\n",
    "    key = cv2.waitKey() & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    if cv2.getWindowProperty(\"Frame\", cv2.WND_PROP_VISIBLE) <1:\n",
    "        break\n",
    "        \n",
    "    if key == ord(\"w\"):\n",
    "        i = (i - 1) % len(detections_sequence)\n",
    "\n",
    "    if key == ord(\"x\"):\n",
    "        i = (i + 1) % len(detections_sequence)\n",
    "        \n",
    "    if key == ord(\"c\"):\n",
    "        display = (display + 1) % 3\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
