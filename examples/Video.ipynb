{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5fcff00",
   "metadata": {},
   "source": [
    "# Video\n",
    "\n",
    "In ByoTrack videos are expected to be Sequences of frames. We support 2D and 3D videos.\n",
    "\n",
    "We use numpy to represent frames: each frame is a numpy array of shape ([D, ]H, W, C). The data type can be floating or integer but most of the codes of ByoTrack will expect the frames to be normalized into [0, 1] and we strongly advise to normalize videos.\n",
    "\n",
    "ByoTrack have its own Video object (*byotrack.Video*) that enables you to read, slice and normalize videos without loading the full video in RAM. In this notebok, we explain how to read, slice, normalize and visualize such Video object in ByoTrack.\n",
    "\n",
    "**NOTE**: In ByoTrack, the Video object can always be directly replaced by a 4D/5D array (T, [D, ]H ,W, C) or a Sequence of array [([D, ]H, W, C), ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import byotrack\n",
    "import byotrack.visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918cc72",
   "metadata": {},
   "source": [
    "## Loading videos\n",
    "\n",
    "A Video can be loaded from a single file (typically mp4 or avi). We support standard format (All those supported by OpenCV) and TIFF stacks.\n",
    "\n",
    "We also support the loading of multiple files. If you give a folder as input path, ByoTrack will try to infer the list of files by itself. It supports most format of images (png, jpeg, gif, .... and also Tiff stack).\n",
    "\n",
    "For more complex cases, you can write your own VideoReader following those already implemented or simply load by yourself the video as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a video from a file:\n",
    "\n",
    "video = byotrack.Video(\"path/to/video.ext\")\n",
    "\n",
    "if video.ndim == 4:\n",
    "    print(\"Video shape: T={}, H={}, W={}, C={}\".format(*video.shape, video.channels))\n",
    "else:\n",
    "    print(\"Video shape: T={}, D={}, H={}, W={}, C={}\".format(*video.shape, video.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a video from a folder:\n",
    "# ByoTrack will find the most common extension in the folder and expect\n",
    "# these files to be the images (sorted alphanumerically).\n",
    "\n",
    "video = byotrack.Video(\"path/to/images/\")\n",
    "\n",
    "if video.ndim == 4:\n",
    "    print(\"Video shape: T={}, H={}, W={}, C={}\".format(*video.shape, video.channels))\n",
    "else:\n",
    "    print(\"Video shape: T={}, D={}, H={}, W={}, C={}\".format(*video.shape, video.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a video from a list of files:\n",
    "# You may also provide the full list of path yourself\n",
    "\n",
    "video = byotrack.Video(\n",
    "    \"path/to/main_folder\",\n",
    "    paths=[\n",
    "        \"path/to/main_folder/first_frame.png\",\n",
    "        \"path/to/main_folder/second_frame.png\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "if video.ndim == 4:\n",
    "    print(\"Video shape: T={}, H={}, W={}, C={}\".format(*video.shape, video.channels))\n",
    "else:\n",
    "    print(\"Video shape: T={}, D={}, H={}, W={}, C={}\".format(*video.shape, video.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14058aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load example videos provided by ByoTrack (See `byotrack.example_data`)\n",
    "# Videos are downloaded in a user data folder and then read.\n",
    "\n",
    "import byotrack.example_data\n",
    "\n",
    "video = byotrack.example_data.hydra_neurons()  # 2D example\n",
    "\n",
    "print(\"Video shape: T={}, H={}, W={}, C={}\".format(*video.shape, video.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the frame shape and dtype\n",
    "\n",
    "print(video[0].shape, video[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953248bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame of a video.\n",
    "# This may not work for uint16 videos where normalization should be apply before visualization\n",
    "\n",
    "frame = video[0]\n",
    "if  video.ndim == 5:  # (T, D, H, W, C) (3D video)\n",
    "    frame = frame[frame.shape[0] // 2]  # Show the frame in the middle of the stack\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bb225",
   "metadata": {},
   "source": [
    "## Channel Selection / Normalization\n",
    "\n",
    "ByoTrack provide some helpers to select and normalize channels in Video objects. It is done on the fly (when frames are required) and it never loads the full video at once.\n",
    "\n",
    "If your video is not a byotrack.Video but a sequence of arrays (or directly a 4D array), you have to handle normalization on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the doc of the Video transformation configuration\n",
    "\n",
    "byotrack.VideoTransformConfig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54875ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VideoTransformConfig\n",
    "\n",
    "transform = byotrack.VideoTransformConfig(\n",
    "    aggregate=True,  # Aggregate channels into a single one\n",
    "    normalize=True,  # Normalize the video into [0, 1]\n",
    "    selected_channel=None,  # None: Average channels, if int, it selects this channel\n",
    "    q_min=0.02,  # We do not normalize using min and max but rather quantile of the intensity distribution\n",
    "    q_max=0.999,  # It enforces q_min to go to 0.0 and q_max to go around 1.0 (depending on smooth_clip)\n",
    "    smooth_clip=0.0,  # Log clipping smoothness for the values that are above q_max (0.0: hard clipping)\n",
    "    compute_stats_on=10  # Number of frames to read to compute the quantiles. The larger the longer it takes.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation. It may take quite a long time to compute the quantiles.\n",
    "\n",
    "video.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09394fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the frame shape and dtype. Notice that channel dimension is kept.\n",
    "\n",
    "print(video[0].shape, video[0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdebf3c",
   "metadata": {},
   "source": [
    "## Temporal and spatial slicing\n",
    "\n",
    "Video objects allows you to slice temporally and spatially the video. Slicing is data intensive, it just creates a new view on the data without modifying it (it does not even load the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the video (number of frames)\n",
    "\n",
    "len(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byotrack supports any temporal slicing\n",
    "\n",
    "# For instance, we slice the first axis (time) using a negative step (the video will be loaded in the reverse order)\n",
    "# from frame 50 to 0. (51 frames)\n",
    "\n",
    "len(video[50::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also add any positional slicing on the height/width to extract a constant square ROI on the video\n",
    "\n",
    "# Let's take the frames from 150 to 250 and centered on the middle of the animal\n",
    "\n",
    "v = video[150:250, 200:-200, 200:-200]\n",
    "v.shape  # 100 frames of shape (448, 624)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56962f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first frame of this sliced video\n",
    "\n",
    "\n",
    "frame = v[0]\n",
    "if  v.ndim == 5:  # (T, D, H, W, C) (3D video)\n",
    "    frame = frame[frame.shape[0] // 2]  # Show the frame in the middle of the stack\n",
    "\n",
    "plt.figure(figsize=(24, 16), dpi=100)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with frame 150 of the original video\n",
    "\n",
    "(v[0] == video[150][200:-200,200:-200]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588501b7",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We provide an interactive visualization code to go through video, detections and tracks.\n",
    "It was developped using open-cv and tested on Linux. Depending on the backend opencv uses, it may have different functionnalities (zooming, screenshots, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the video with opencv\n",
    "# Use w/x to move forward in time (or space to run/pause the video)\n",
    "# Use b/n to move inside the stack (For 3D videos)\n",
    "# Use v to switch on/off the display of the video\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can display a sliced video\n",
    "# First focus on the 300 first frames, then go backward in time (5 frames at a time) and flip the vertical axis\n",
    "\n",
    "byotrack.visualize.InteractiveVisualizer(video[:300][::-5, ::-1]).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996b955",
   "metadata": {},
   "source": [
    "## Tiff videos specificities\n",
    "\n",
    "We support the tiff format for videos. We try to infer axes and shapes from the metadata and what the user is trying to do, but our TiffVideoReader accepts some extra arguments to overwrite this.\n",
    "\n",
    "Also for large 3D videos, we implemented an on-read slicing, allowing to load only the part of the frame your interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the doc of the TiffVideoReader\n",
    "\n",
    "byotrack.video.reader.TiffVideoReader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8218eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the axis of the tiff stack\n",
    "# ByoTrack reads the metadata to find the name of each axis. This allows to manually overwrite this behavior.\n",
    "# We use ImageJ convention for tiff axes: T for time, Z for stack, Y for height, X for width and C/S for channels.\n",
    "# Most tiff are usually sorted in a TZYX order (without channels)\n",
    "\n",
    "# Default loading\n",
    "video = byotrack.Video(\"path/to/video.tiff\")\n",
    "\n",
    "print(\"Video shape: T={}, D={}, H={}, W={}, C={}\".format(*video.shape, video.channels))\n",
    "\n",
    "# Let's interpret the first axis as channels no matter the metadata in the tiff.\n",
    "video = byotrack.Video(\"path/to/video.tiff\", axes=\"CTZXY\")\n",
    "\n",
    "print(\"Video shape: T={}, D={}, H={}, W={}, C={}\".format(*video.shape, video.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-read slicing the axis of the tiff stack\n",
    "# This allows to reduce memory and time consumption for video loading when large frames are involved\n",
    "\n",
    "# For instance here, we downscale the Z axis by 2, and select only the second channel at read time.\n",
    "video = byotrack.Video(\"path/to/video.tiff\", ax_slice={\"Z\": slice(None, None, 2), \"C\": slice(1, 2)})\n",
    "\n",
    "print(\"Video shape: T={}, D={}, H={}, W={}, C={}\".format(*video.shape, video.channels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
